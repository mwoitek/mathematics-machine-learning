{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "We will implement the PCA algorithm. We will first implement PCA, then apply it\n",
    "(once again) to the MNIST digit dataset.\n",
    "\n",
    "## Learning objective\n",
    "1. Write code that implements PCA.\n",
    "2. Write code that implements PCA for high-dimensional datasets\n",
    "\n",
    "Let's first import the packages we need for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE: DO NOT EDIT\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy.testing import assert_allclose\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_mnist\n",
    "\n",
    "\n",
    "MNIST = load_mnist('./')\n",
    "images, labels = MNIST['data'], MNIST['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a digit from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(images[0].reshape(28, 28), cmap='gray');\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Now we will implement PCA. Before we do that, let's pause for a moment and\n",
    "think about the steps for performing PCA. Assume that we are performing PCA on\n",
    "some dataset $\\boldsymbol X$ for $M$ principal components.\n",
    "We then need to perform the following steps, which we break into parts:\n",
    "\n",
    "1. Data normalization (`normalize`).\n",
    "2. Find eigenvalues and corresponding eigenvectors for the covariance matrix\n",
    "$S$.\n",
    "   Sort by the largest eigenvalues and the corresponding eigenvectors (`eig`).\n",
    "3. Compute the orthogonal projection matrix and use that to project the data\n",
    "onto the subspace spanned by the eigenvectors.\n",
    "\n",
    "### Data normalization `normalize`\n",
    "\n",
    "We will first implement the data normalization mentioned above.\n",
    "\n",
    "Before we implement the main steps of PCA, we will need to do some data\n",
    "preprocessing.\n",
    "\n",
    "To preprocess the dataset for PCA, we will make sure that the dataset has zero\n",
    "mean. Given a dataset $\\mathbf{X}$, we will subtract the mean vector from each\n",
    "row of the dataset to obtain a zero-mean dataset $\\overline{\\mathbf{X}}$. In the\n",
    "first part of this notebook, you will implement `normalize` to do that.\n",
    "\n",
    "To work with images, it's also a common practice to convert the pixels from\n",
    "unsigned integer 8 (uint8) encoding to a floating point number representation\n",
    "between 0-1. We will do this conversion for you for the MNIST dataset so that\n",
    "you don't have to worry about it.\n",
    "\n",
    "Data normalization is a common practice. More details can be found in\n",
    "[Data Normalization or Feature\n",
    "Scaling](https://en.wikipedia.org/wiki/Feature_scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def normalize(X):\n",
    "    \"\"\"Normalize the dataset X to have zero mean.\n",
    "\n",
    "    Arguments:\n",
    "        X: Dataset. Array of shape (N, D).\n",
    "\n",
    "    Returns:\n",
    "        Xbar: Normalized dataset with zero mean. Array of shape (N, D).\n",
    "        mu: Sample mean of the dataset. Array of shape (D,).\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Compute the mean of X:\n",
    "    mu = np.mean(X, axis=0)\n",
    "\n",
    "    # Compute the normalized dataset:\n",
    "    Xbar = X - mu\n",
    "\n",
    "    return Xbar, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation of 'normalize':\n",
    "\n",
    "# Test case 1:\n",
    "X0 = np.array([[0.0, 0.0],\n",
    "               [1.0, 1.0],\n",
    "               [2.0, 2.0]])\n",
    "X0_normalize, X0_mean = normalize(X0)\n",
    "assert_allclose(np.mean(X0_normalize, axis=0), np.zeros((2,)))\n",
    "assert_allclose(X0_mean, np.array([1.0, 1.0]))\n",
    "assert_allclose(normalize(X0_normalize)[0], X0_normalize)\n",
    "\n",
    "# Test case 2:\n",
    "X0 = np.array([[0.0, 0.0],\n",
    "               [1.0, 0.0],\n",
    "               [2.0, 0.0]])\n",
    "X0_normalize, X0_mean = normalize(X0)\n",
    "assert_allclose(np.mean(X0_normalize, axis=0), np.zeros((2,)))\n",
    "assert_allclose(X0_mean, np.array([1.0, 0.0]))\n",
    "assert_allclose(normalize(X0_normalize)[0], X0_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute eigenvalues and eigenvectors `eig`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def eig(S):\n",
    "    \"\"\"Compute the eigenvalues and the corresponding eigenvectors for the\n",
    "       covariance matrix S.\n",
    "\n",
    "    Arguments:\n",
    "        S: Covariance matrix.\n",
    "\n",
    "    Returns:\n",
    "        eigvals: Eigenvalues of the covariance matrix.\n",
    "        eigvecs: Eigenvectors of the covariance matrix.\n",
    "        The eigenvalues and eigenvectors are sorted in descending order of the\n",
    "        eigenvalues.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the eigenvalues and eigenvectors of the covariance matrix:\n",
    "    eigvals, eigvecs = np.linalg.eig(S)\n",
    "\n",
    "    # Sort the eigenvalues and eigenvectors in descending order according to\n",
    "    # the eigenvalues:\n",
    "    sort_indices = np.argsort(eigvals)[::-1]\n",
    "\n",
    "    return eigvals[sort_indices], eigvecs[:, sort_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some test cases for implementing `eig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_eigenvectors(B):\n",
    "    \"\"\"Normalize the eigenvectors to have unit length. Also flip the direction\n",
    "       of the eigenvectors whose first component is negative.\"\"\"\n",
    "\n",
    "    # Normalize the eigenvectors:\n",
    "    B_normalized = B / np.linalg.norm(B, axis=0)\n",
    "\n",
    "    # If necessary, flip the direction of the eigenvectors:\n",
    "    for i in range(B.shape[1]):\n",
    "        if B_normalized[0, i] < 0:\n",
    "            B_normalized[:, i] *= -1\n",
    "\n",
    "    return B_normalized\n",
    "\n",
    "\n",
    "# Test the implementation of 'eig':\n",
    "\n",
    "A = np.array([[3, 2],\n",
    "              [2, 3]])\n",
    "expected_eigenvalues  = np.array([5, 1])\n",
    "expected_eigenvectors = np.array([[0.70710678, -0.70710678],\n",
    "                                  [0.70710678,  0.70710678]])\n",
    "actual_eigenvalues, actual_eigenvectors = eig(A)\n",
    "\n",
    "# Check that the eigenvalues match:\n",
    "assert_allclose(actual_eigenvalues, expected_eigenvalues)\n",
    "\n",
    "# Check that the eigenvectors match:\n",
    "assert_allclose(\n",
    "    _normalize_eigenvectors(actual_eigenvectors),\n",
    "    _normalize_eigenvectors(expected_eigenvectors)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute projection matrix\n",
    "\n",
    "Next given a orthonormal basis spanned by the eigenvectors,\n",
    "we will compute the projection matrix. This should be the same\n",
    "as what you have done last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def projection_matrix(B):\n",
    "    \"\"\"Compute the projection matrix onto the subspace spanned by the columns of B.\n",
    "\n",
    "    Arguments:\n",
    "        B: Array of shape (D, M).\n",
    "           The columns of B correspond to a basis for the subspace.\n",
    "\n",
    "    Returns:\n",
    "        P: Array of shape (D, D).\n",
    "           Projection matrix onto the subspace spanned by the columns of B.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transpose of B:\n",
    "    trans_B = np.transpose(B)\n",
    "\n",
    "    # Compute the projection matrix onto the subspace spanned by the columns of B:\n",
    "    P = np.linalg.inv(np.matmul(trans_B, B))\n",
    "    P = np.matmul(B, P)\n",
    "    P = np.matmul(P, trans_B)\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation of 'projection_matrix':\n",
    "B = np.array([[1, 0],\n",
    "              [1, 1],\n",
    "              [1, 2]])\n",
    "exp_proj_matrix = np.array([[5,  2, -1],\n",
    "                            [2,  2,  2],\n",
    "                            [-1, 2,  5]]) / 6\n",
    "assert_allclose(projection_matrix(B), exp_proj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def PCA(X, num_components):\n",
    "    \"\"\"Implementation of the principal component analysis.\n",
    "\n",
    "    Arguments:\n",
    "        X: Array of size (N, D).\n",
    "           D is the dimension of the data, and N is the number of datapoints.\n",
    "        num_components: Number of principal components to use.\n",
    "\n",
    "    Returns:\n",
    "        reconst: Array of size (N, D). Reconstructed data.\n",
    "        mean: Array of size (D,). Sample mean of X.\n",
    "        principal_vals: Array of size (num_components,). Principal values.\n",
    "        principal_components: Array of size (D, num_components). Principal components.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the dataset X to have zero mean:\n",
    "    X_normalized, mean = normalize(X)\n",
    "\n",
    "    # Compute the covariance matrix S:\n",
    "    S = np.cov(X_normalized, rowvar=False, bias=True)\n",
    "\n",
    "    # Find eigenvalues and eigenvectors for S:\n",
    "    eig_vals, eig_vecs = eig(S)\n",
    "\n",
    "    # Take the principal values and components:\n",
    "    principal_vals       = eig_vals[:num_components]\n",
    "    principal_components = eig_vecs[:, :num_components]\n",
    "\n",
    "    # Compute the projection matrix onto the subspace spanned by the principal\n",
    "    # components:\n",
    "    P = projection_matrix(principal_components)\n",
    "\n",
    "    # Reconstruct the data:\n",
    "    reconst = (X_normalized @ P + mean).astype(np.float_)\n",
    "\n",
    "    return reconst, mean, principal_vals, principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None, label=None):\n",
    "    \"\"\"Draw a vector from v0 to v1.\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(\n",
    "        arrowstyle='->',\n",
    "        linewidth=2,\n",
    "        shrinkA=0,\n",
    "        shrinkB=0,\n",
    "        color='k'\n",
    "    )\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some test cases that check the implementation of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation of 'PCA':\n",
    "\n",
    "# Generate a dataset X from a 2D Gaussian distribution:\n",
    "D = 2\n",
    "N = 10\n",
    "mvn = scipy.stats.multivariate_normal(\n",
    "    mean=np.ones(D, dtype=np.float_),\n",
    "    cov=np.array([[1.0, 0.8],\n",
    "                  [0.8, 1.0]], dtype=np.float_)\n",
    ")\n",
    "X = mvn.rvs((N,), random_state=np.random.RandomState(0))\n",
    "\n",
    "# Check that the arrays returned by 'PCA' have the correct shape:\n",
    "reconst, m, pv, pc    = PCA(X, 1)\n",
    "assert reconst.shape == X.shape\n",
    "assert m.shape       == (D,)\n",
    "assert pv.shape      == (1,)\n",
    "assert pc.shape      == (D, 1)\n",
    "\n",
    "# Check that, for num_components == D, the reconstructed dataset and the\n",
    "# original dataset are equal:\n",
    "reconst, m, pv, pc    = PCA(X, D)\n",
    "assert reconst.shape == X.shape\n",
    "assert m.shape       == (D,)\n",
    "assert pv.shape      == (D,)\n",
    "assert pc.shape      == (D, D)\n",
    "assert_allclose(reconst, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize PCA\n",
    "We will first visualize what PCA does on a 2D toy dataset. You can use the\n",
    "visualization\n",
    "below to get better intuition about what PCA does and use it to debug your code\n",
    "above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "N = 100\n",
    "num_components = 1\n",
    "mvn = scipy.stats.multivariate_normal(\n",
    "    mean=np.ones(D, dtype=np.float_),\n",
    "    cov=np.array([[1.0, 0.8],\n",
    "                  [0.8, 1.0]], dtype=np.float_)\n",
    ")\n",
    "X = mvn.rvs((N,), random_state=np.random.RandomState(0))\n",
    "X_reconst, mean, principal_values, principal_components = PCA(X, num_components)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(X[:, 0], X[:, 1], label='Original')\n",
    "for principal_variance, principal_component in zip(principal_values, principal_components.T):\n",
    "    draw_vector(\n",
    "        mean,\n",
    "        mean + np.sqrt(principal_variance) * principal_component,\n",
    "        ax=ax\n",
    "    )\n",
    "ax.scatter(X_reconst[:, 0], X_reconst[:, 1], label='Reconstructed')\n",
    "plt.axis('equal');\n",
    "plt.legend();\n",
    "ax.set(xlabel='$\\mathbf{x}_0$', ylabel='$\\mathbf{x}_1$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare our PCA implementation with the implementation in scikit-\n",
    "learn (a popular machine learning library in Python that includes implementation\n",
    "of PCA)\n",
    "to see\n",
    "if we get identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as SKPCA\n",
    "\n",
    "\n",
    "# Generate random data:\n",
    "random = np.random.RandomState(0)\n",
    "X = random.randn(10, 5)\n",
    "\n",
    "# Compare the results returned by 'PCA' and 'SKPCA':\n",
    "msg = 'Difference in reconstruction for num_components = {}: {}'\n",
    "for num_component in range(1, 4):\n",
    "\n",
    "    # Compute the reconstructed dataset by using scikit-learn's implementation\n",
    "    # of PCA:\n",
    "    pca = SKPCA(n_components=num_component, svd_solver='full')\n",
    "    sklearn_reconst = pca.inverse_transform(pca.fit_transform(X))\n",
    "\n",
    "    # Compute the reconstructed dataset by using our implementation of PCA:\n",
    "    reconst, _, _, _ = PCA(X, num_component)\n",
    "\n",
    "    # Compute a measure of the difference between 'reconst' and\n",
    "    # 'sklearn_reconst'. This difference should be very small.\n",
    "    diff = np.square(reconst - sklearn_reconst).sum()\n",
    "    print(msg.format(num_component, diff))\n",
    "\n",
    "    assert_allclose(reconst, sklearn_reconst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for MNIST digits\n",
    "\n",
    "Once you have implemented PCA correctly, it's time to apply to the MNIST\n",
    "dataset.\n",
    "First, we will do some preprocessing of the data to get it into a good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preprocessing of the data:\n",
    "NUM_DATAPOINTS = 1000\n",
    "X = (images.reshape(-1, 28 * 28)[:NUM_DATAPOINTS]) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst, _, _, _ = PCA(X, 10)\n",
    "num_images_to_show = 10\n",
    "reconst_images = np.reshape(reconst[:num_images_to_show], (-1, 28, 28))\n",
    "fig, ax = plt.subplots(2, 1, figsize=(3 * num_images_to_show, 3))\n",
    "ax[0].imshow(np.concatenate(np.reshape(X[:num_images_to_show], (-1, 28, 28)), -1), cmap='gray')\n",
    "ax[1].imshow(np.concatenate(reconst_images, -1), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greater number of of principal components we use, the smaller will our\n",
    "reconstruction\n",
    "error be. Now, let's answer the following question:\n",
    "\n",
    "\n",
    "> How many principal components do we need\n",
    "> in order to reach a Mean Squared Error (MSE) of less than $10.0$ for our\n",
    "dataset?\n",
    "\n",
    "\n",
    "We have provided a function in the next cell which computes the mean squared\n",
    "error (MSE), which will be useful for answering the question above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predict, actual):\n",
    "    \"\"\"Compute the mean squared error.\"\"\"\n",
    "    return np.square(predict - actual).sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = []\n",
    "loss = []\n",
    "\n",
    "# Iterate over different numbers of principal components, and compute the MSE:\n",
    "for num_component in range(1, 100, 5):\n",
    "    reconst, _, _, _ = PCA(X, num_component)\n",
    "    reconstructions.append(reconst)\n",
    "    error = mse(reconst, X)\n",
    "    loss.append((num_component, error))\n",
    "\n",
    "reconstructions = np.array(reconstructions)\n",
    "loss = np.array(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create a table showing the number of principal components and the\n",
    "# corresponding MSE:\n",
    "pd.DataFrame(loss, columns=['No. of Components', 'MSE']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also put these numbers into perspective by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss[:, 0], loss[:, 1]);\n",
    "ax.axhline(10, linestyle='--', color='r', linewidth=2)\n",
    "ax.xaxis.set_ticks(np.arange(1, 100, 5));\n",
    "ax.set(\n",
    "    xlabel='No. of Components',\n",
    "    ylabel='MSE',\n",
    "    title='MSE vs Number of Principal Components'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But _numbers don't tell us everything_! Just what does it mean _qualitatively_\n",
    "for the loss to decrease from around\n",
    "$45.0$ to less than $10.0$?\n",
    "\n",
    "Let's find out! In the next cell, we draw the leftmost image is the original\n",
    "dight. Then we show the reconstruction of the image on the right, in descending\n",
    "number of principal components used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(image_idx=(0, 1000))\n",
    "def show_num_components_reconst(image_idx):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    actual = X[image_idx]\n",
    "    x = np.concatenate([actual[np.newaxis, :], reconstructions[:, image_idx]])\n",
    "    ax.imshow(np.hstack(x.reshape(-1, 28, 28)[np.arange(10)]), cmap='gray');\n",
    "    ax.axvline(28, color='orange', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also browse through the reconstructions for other digits. Once again,\n",
    "`interact` becomes handy for visualizing the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(i=(0, 10))\n",
    "def show_pca_digits(i=1):\n",
    "    \"\"\"Show the i-th digit and its reconstruction.\"\"\"\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    actual_sample = X[i].reshape(28, 28)\n",
    "    reconst_sample = (reconst[i, :]).reshape(28, 28)\n",
    "    plt.imshow(np.hstack([actual_sample, reconst_sample]), cmap='gray')\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for high-dimensional datasets\n",
    "\n",
    "Sometimes, the dimensionality of our dataset may be larger than the number of\n",
    "samples we\n",
    "have. Then it might be inefficient to perform PCA with your implementation\n",
    "above. Instead,\n",
    "as mentioned in the lectures, you can implement PCA in a more efficient manner,\n",
    "which we\n",
    "call \"PCA for high dimensional data\" (PCA_high_dim).\n",
    "\n",
    "Below are the steps for performing PCA for high dimensional dataset\n",
    "1. Normalize the dataset matrix $X$ to obtain $\\overline{X}$ that has zero mean.\n",
    "2. Compute the matrix $\\overline{X}\\overline{X}^T$ (a $N$ by $N$ matrix with\n",
    "$N\\ll D$)\n",
    "3. Compute eigenvalues $\\lambda$s and eigenvectors $V$ for\n",
    "$\\overline{X}\\overline{X}^T$ with shape (N, N). Compare this with computing the\n",
    "eigenspectrum of $\\overline{X}^T\\overline{X}$ which has shape (D, D), when $N\\ll\n",
    "D$,\n",
    "computation of the eigenspectrum of $\\overline{X}\\overline{X}^T$ will be\n",
    "computationally less expensive.\n",
    "4. Compute the eigenvectors for the original covariance matrix as\n",
    "$\\overline{X}^TV$. Choose the eigenvectors associated with the `n` largest\n",
    "eigenvalues to be the basis of the principal subspace $U$.\n",
    "    1. Notice that $\\overline{X}^TV$ would give a matrix of shape (D, N) but the\n",
    "eigenvectors beyond the Dth column will have eigenvalues of 0, so it is safe to\n",
    "drop any columns beyond the D'th dimension.\n",
    "    2. Also note that the columns of $U$ will not be unit-length if we pre-\n",
    "multiply $V$ with $\\overline{X}^T$, so we will have to normalize the columns of\n",
    "$U$ so that they have unit-length to be consistent with the `PCA` implementation\n",
    "above.\n",
    "5. Compute the orthogonal projection of the data onto the subspace spanned by\n",
    "columns of $U$.\n",
    "\n",
    "Functions you wrote for earlier assignments will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: DO NOT EDIT THIS LINE\n",
    "def PCA_high_dim(X, num_components):\n",
    "    \"\"\"Implementation of the principal component analysis\n",
    "       for small sample size but high-dimensional features.\n",
    "\n",
    "    Arguments:\n",
    "        X: Array of size (N, D).\n",
    "           D is the dimension of the data, and N is the number of datapoints.\n",
    "        num_components: Number of principal components to use.\n",
    "\n",
    "    Returns:\n",
    "        reconst: Array of size (N, D). Reconstructed data.\n",
    "        mean: Array of size (D,). Sample mean of X.\n",
    "        principal_vals: Array of size (num_components,). Principal values.\n",
    "        principal_components: Array of size (D, num_components). Principal components.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the dataset X to have zero mean:\n",
    "    X_normalized, mean = normalize(X)\n",
    "\n",
    "    # Transpose of X_normalized:\n",
    "    trans_X_norm = X_normalized.T\n",
    "\n",
    "    # Compute the matrix mentioned in step 2:\n",
    "    M = (X_normalized @ trans_X_norm) / X.shape[0]\n",
    "\n",
    "    # Find eigenvalues and eigenvectors for M:\n",
    "    eig_vals, eig_vecs = eig(M)\n",
    "\n",
    "    # Find eigenvectors for the covariance matrix S:\n",
    "    eig_vecs = trans_X_norm @ eig_vecs\n",
    "\n",
    "    # Normalize the eigenvectors to have unit length:\n",
    "    eig_vecs /= np.linalg.norm(eig_vecs, axis=0)\n",
    "\n",
    "    # Take the principal values and components:\n",
    "    principal_vals       = eig_vals[:num_components]\n",
    "    principal_components = eig_vecs[:, :num_components]\n",
    "\n",
    "    # Compute the projection matrix onto the subspace spanned by the principal\n",
    "    # components:\n",
    "    P = projection_matrix(principal_components)\n",
    "\n",
    "    # Reconstruct the data:\n",
    "    reconst = (X_normalized @ P + mean).astype(np.float_)\n",
    "\n",
    "    return reconst, mean, principal_vals, principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the same dataset, `PCA_high_dim` and `PCA` should give the same output.\n",
    "Assuming we have implemented `PCA`, correctly, we can then use `PCA` to test the\n",
    "correctness\n",
    "of `PCA_high_dim`. Given the same dataset, `PCA` and `PCA_high_dim` should give\n",
    "identical results.\n",
    "\n",
    "We can use this __invariant__\n",
    "to test our implementation of PCA_high_dim, assuming that we have correctly\n",
    "implemented `PCA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation of 'PCA_high_dim':\n",
    "\n",
    "# Generate random data:\n",
    "random = np.random.RandomState(0)\n",
    "X = random.randn(5, 4)\n",
    "\n",
    "# Check that the results returned by 'PCA' and 'PCA_high_dim' are identical:\n",
    "pca_rec, pca_mean, pca_pvs, pca_pcs = PCA(X, 2)\n",
    "pca_hd_rec, pca_hd_mean, pca_hd_pvs, pca_hd_pcs = PCA_high_dim(X, 2)\n",
    "assert_allclose(pca_rec, pca_hd_rec)\n",
    "assert_allclose(pca_mean, pca_hd_mean)\n",
    "assert_allclose(pca_pvs, pca_hd_pvs)\n",
    "assert_allclose(\n",
    "    _normalize_eigenvectors(pca_pcs),\n",
    "    _normalize_eigenvectors(pca_hd_pcs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Congratulations_! You have now learned how PCA works!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
